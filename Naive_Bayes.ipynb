{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
        "\n",
        "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
          "inputWidgets": {},
          "title": ""
        },
        "id": "GTlYkK52S8Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File location and type\n",
        "file_location = \"/FileStore/tables/data_f.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "df = df.drop('summary')\n",
        "df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "inputWidgets": {},
          "title": ""
        },
        "id": "y2LKJhmlS8Lt",
        "outputId": "8419c07e-8431-4cbc-a81f-99f27e0268b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n|             overall|          reviewText|\n+--------------------+--------------------+\n|                   5|The stained glass...|\n|                   5|My 11 y.o. loved ...|\n|Dragons and Wizar...| that make it \"\"s...|\n|Even the perfecti...| so it tends to b...|\n|                   5|The pictures are ...|\n|                   5|I absolutely love...|\n|                   5|          I love it!|\n|                   5|MY HUSBAND LOVED ...|\n|                   5|             love it|\n|                   4|                cool|\n|                   5|Exactly as descri...|\n|                   5|Sometimes you nee...|\n|I love all of the...| Great 30 minute toy|\n|                   5|These little book...|\n|                   2|This is indeed a ...|\n|           Why? Well| there's really n...|\n|Not recommended u...|                null|\n|           Plus side| small enough to ...|\n|                   4|I bought several ...|\n|                   1|total waste of mo...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.filter(~col(\"overall\").rlike('\\D'))\n",
        "df.show(5)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "ff5da813-711f-4b11-8077-8f333b7f025f",
          "inputWidgets": {},
          "title": ""
        },
        "id": "lZdiURmmS8Ly",
        "outputId": "e5fb2ff5-acdb-4138-ee08-6d82c7a55c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n|overall|          reviewText|\n+-------+--------------------+\n|      5|The stained glass...|\n|      5|My 11 y.o. loved ...|\n|      5|The pictures are ...|\n|      5|I absolutely love...|\n|      5|          I love it!|\n+-------+--------------------+\nonly showing top 5 rows\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "data = df.select(col(\"reviewText\"), col(\"overall\"))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "4230a552-2142-4f17-a4fd-e133b0a5cc5c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "7EFQ1XFHS8L0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count the number of null values in each column\n",
        "null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
        "\n",
        "# Display the result\n",
        "null_counts.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "3d437931-4f07-4266-9193-683691815e28",
          "inputWidgets": {},
          "title": ""
        },
        "id": "7e6bMBK3S8L0",
        "outputId": "b9187cd7-bad8-4992-8c73-84273db871b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n|reviewText|overall|\n+----------+-------+\n|         9|      0|\n+----------+-------+\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# drop any rows with null values\n",
        "data = data.na.drop()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "025e8ab4-c77f-454b-90a1-8877544e9633",
          "inputWidgets": {},
          "title": ""
        },
        "id": "9MUeu0TGS8L1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count the number of null values in each column\n",
        "null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
        "\n",
        "# Display the result\n",
        "null_counts.show()\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "d09e050d-4559-4b7b-b11a-582a6bee9bf7",
          "inputWidgets": {},
          "title": ""
        },
        "id": "AyMZ9WfDS8L2",
        "outputId": "8d27b5f4-5e16-4f73-c427-8c00955714e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n|reviewText|overall|\n+----------+-------+\n|         0|      0|\n+----------+-------+\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with the dataset\n",
        "\n",
        "# Specify the fraction of rows to delete\n",
        "fraction_to_delete = 0.45  # 50% of the rows will be deleted\n",
        "\n",
        "# Randomly delete rows\n",
        "randomly_deleted_df = data.sample(fraction=1 - fraction_to_delete, seed=42)\n",
        "\n",
        "# Display the remaining DataFrame\n",
        "remaining_df = randomly_deleted_df\n",
        "remaining_df.show()\n",
        "\n",
        "# If you want to overwrite the original DataFrame with the remaining one, you can reassign the variable:\n",
        "data = remaining_df"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "363a8e67-d3ec-412b-80ba-8cf7d43ccf3e",
          "inputWidgets": {},
          "title": ""
        },
        "id": "ubJ1djgNS8L3",
        "outputId": "fa078bcc-942f-4a4d-c212-98e89ee6a752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n|          reviewText|overall|\n+--------------------+-------+\n|My 11 y.o. loved ...|      5|\n|I absolutely love...|      5|\n|MY HUSBAND LOVED ...|      5|\n|                cool|      4|\n|These little book...|      5|\n|I bought several ...|      4|\n|This is pretty mu...|      3|\n|its a cute little...|      4|\n|They were ok but ...|      2|\n|Great fun for my ...|      5|\n|          great book|      5|\n|   Grandson loved it|      5|\n|This is a small book|      5|\n|\"i think there ar...|      3|\n|\"This was a great...|      5|\n|I bought this as ...|      4|\n|Not sure why ther...|      2|\n|We bought this to...|      4|\n|This book is smal...|      5|\n|My 4 year old lov...|      5|\n+--------------------+-------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "0c540742-29b8-4884-8ab0-d6da466733c3",
          "inputWidgets": {},
          "title": ""
        },
        "id": "J1RsLs5KS8L5",
        "outputId": "80332414-0069-4751-d3a3-c62370bdaba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[27]: 1006439"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
        "\n",
        "# Define the stop words remover\n",
        "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "\n",
        "# Define the hashing term frequency (HTF) vectorizer\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
        "\n",
        "# Define the inverse document frequency (IDF) transformer\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Define the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, idf])\n",
        "\n",
        "# Fit the preprocessing pipeline to the data and transform the data\n",
        "preprocessed_data = preprocessing_pipeline.fit(data).transform(data).select(\"features\", \"overall\")\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "61d59fe7-2d4f-48e4-9ee0-ea36446aad4a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "MRkLUvR0S8L6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Convert the 'overall' column to a numeric type\n",
        "preprocessed_data = preprocessed_data.withColumn('overall', col('overall').cast('double'))\n",
        "\n",
        "# Check the schema again\n",
        "preprocessed_data.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "ffca761a-a430-4847-96f2-c8b6e0233e1a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "wz3qu0SCS8L7",
        "outputId": "f2049f33-5482-4b27-803f-1a3fcda97e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- features: vector (nullable = true)\n |-- overall: double (nullable = true)\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocessed_data.repartition(5000)\n",
        "(training_data, test_data) = preprocessed_data.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "32227ca9-d16f-43e0-a923-39976c187101",
          "inputWidgets": {},
          "title": ""
        },
        "id": "Lb-hbGbES8L8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Define the Naive Bayes model\n",
        "nb = NaiveBayes(labelCol=\"overall\", smoothing=1.0)\n",
        "\n",
        "# Cache the training data to avoid recalculating it multiple times\n",
        "training_data.cache()\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb_model = nb.fit(training_data)\n",
        "\n",
        "# Unpersist the training data to release memory\n",
        "training_data.unpersist()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using accuracy metric\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"overall\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "b99f6f7c-1bd2-4aa3-a079-ce6c928341fc",
          "inputWidgets": {},
          "title": ""
        },
        "id": "hIOCDzP0S8L8",
        "outputId": "c5d40850-fcfe-41c8-e86f-2e186b8789d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.09492557487733852\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Convert the 'overall' column to a numeric type\n",
        "data = data.withColumn('overall', col('overall').cast('double'))\n",
        "\n",
        "# Check the schema again\n",
        "data.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "59f4ee77-9496-4265-a6e6-3e0a0423d2a0",
          "inputWidgets": {},
          "title": ""
        },
        "id": "c8hb6LXDS8L9",
        "outputId": "0bfb62aa-fe43-49e7-a059-89ddc293da77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- reviewText: string (nullable = true)\n |-- overall: double (nullable = true)\n\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec, RegexTokenizer\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Define a regular expression tokenizer to split the text into words\n",
        "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"word\", pattern=\"\\\\W\")\n",
        "\n",
        "# Apply the tokenizer to the data\n",
        "words_df = tokenizer.transform(data).na.drop(subset=[\"word\"])\n",
        "\n",
        "# Check if there are any null or missing values in the \"word\" column\n",
        "if words_df.filter(col(\"word\").isNull() | (col(\"word\") == \"\")).count() > 0:\n",
        "    print(\"Warning: Null or missing values found in the 'word' column. Please handle them before proceeding.\")\n",
        "\n",
        "# Learn a Word2Vec model on the text data\n",
        "word2vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"word\", outputCol=\"features\")\n",
        "word2vec_model = word2vec.fit(words_df)\n",
        "\n",
        "# Transform the data using the Word2Vec model\n",
        "word2vec_df = word2vec_model.transform(words_df)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "(training_data, test_data) = word2vec_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a Logistic Regression model on the data\n",
        "lr = LogisticRegression(labelCol=\"overall\", maxIter=10, regParam=0.01, elasticNetParam=0)\n",
        "lr_model = lr.fit(training_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using accuracy metric\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"overall\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "42e40088-1849-48fa-ba01-a066bffc42ae",
          "inputWidgets": {},
          "title": ""
        },
        "id": "Jmv0D83cS8L-",
        "outputId": "1054085e-4a47-4738-c5ba-9d4c40e4a88a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-931624360125395>:14\u001b[0m\n\u001b[1;32m     10\u001b[0m # Apply the tokenizer to the data\n\u001b[1;32m     11\u001b[0m words_df = tokenizer.transform(data).na.drop(subset=[\"word\"])\n\u001b[1;32m     13\u001b[0m # Check if there are any null or missing values in the \"words\" column\n\u001b[0;32m---> 14\u001b[0m # Check if there are any null or missing values in the \"word\" column\n\u001b[1;32m     15\u001b[0m if words_df.filter(col(\"word\").isNull() | col(\"word\").isEmpty()).count() > 0:\n\u001b[1;32m     16\u001b[0m     print(\"Warning: Null or missing values found in the 'word' column. Please handle them before proceeding.\")\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3125\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3123\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3125\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3128\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3129\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3130\u001b[0m     )\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(word = )\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").;\n'Filter (isnull(word#5093) OR (word#5093 = ))\n+- Filter atleastnnonnulls(1, word#5093)\n   +- Project [overall#5023, reviewText#5024, summary#5025, words#5032, UDF(reviewText#5024) AS word#5093]\n      +- Project [overall#5023, reviewText#5024, summary#5025, UDF(reviewText#5024) AS words#5032]\n         +- Relation [overall#5023,reviewText#5024,summary#5025] csv\n",
              "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(word = )\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").;\n'Filter (isnull(word#5093) OR (word#5093 = ))\n+- Filter atleastnnonnulls(1, word#5093)\n   +- Project [overall#5023, reviewText#5024, summary#5025, words#5032, UDF(reviewText#5024) AS word#5093]\n      +- Project [overall#5023, reviewText#5024, summary#5025, UDF(reviewText#5024) AS words#5032]\n         +- Relation [overall#5023,reviewText#5024,summary#5025] csv\n",
              "metadata": {},
              "errorTraceType": "ansi",
              "type": "ipynbError",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-931624360125395>:14\u001b[0m\n",
              "\u001b[1;32m     10\u001b[0m # Apply the tokenizer to the data\n",
              "\u001b[1;32m     11\u001b[0m words_df = tokenizer.transform(data).na.drop(subset=[\"word\"])\n",
              "\u001b[1;32m     13\u001b[0m # Check if there are any null or missing values in the \"words\" column\n",
              "\u001b[0;32m---> 14\u001b[0m # Check if there are any null or missing values in the \"word\" column\n",
              "\u001b[1;32m     15\u001b[0m if words_df.filter(col(\"word\").isNull() | col(\"word\").isEmpty()).count() > 0:\n",
              "\u001b[1;32m     16\u001b[0m     print(\"Warning: Null or missing values found in the 'word' column. Please handle them before proceeding.\")\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
              "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
              "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
              "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
              "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
              "\u001b[1;32m     51\u001b[0m     )\n",
              "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3125\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n",
              "\u001b[1;32m   3123\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n",
              "\u001b[1;32m   3124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n",
              "\u001b[0;32m-> 3125\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
              "\u001b[1;32m   3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
              "\u001b[1;32m   3128\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
              "\u001b[1;32m   3129\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
              "\u001b[1;32m   3130\u001b[0m     )\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
              "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
              "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
              "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
              "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
              "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
              "\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
              "\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
              "\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
              "\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
              "\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
              "\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
              "\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
              "\n",
              "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(word = )\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"ARRAY<STRING>\" and \"STRING\").;\n",
              "'Filter (isnull(word#5093) OR (word#5093 = ))\n",
              "+- Filter atleastnnonnulls(1, word#5093)\n",
              "   +- Project [overall#5023, reviewText#5024, summary#5025, words#5032, UDF(reviewText#5024) AS word#5093]\n",
              "      +- Project [overall#5023, reviewText#5024, summary#5025, UDF(reviewText#5024) AS words#5032]\n",
              "         +- Relation [overall#5023,reviewText#5024,summary#5025] csv\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"overall\").rdd.map(lambda x: (x.prediction, x.overall))\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "7ee39d15-ad33-43fe-b87f-9b268d25196a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "kQV9pMHzS8L_",
        "outputId": "23173225-5eb2-4e44-f662-3bebe060e86a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-931624360125386>:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictionAndLabels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x\u001b[38;5;241m.\u001b[39mprediction, x\u001b[38;5;241m.\u001b[39moverall))\n\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MulticlassMetrics(predictionAndLabels)\n\u001b[1;32m      4\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mconfusionMatrix()\u001b[38;5;241m.\u001b[39mtoArray()\n\n\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined",
              "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'predictions' is not defined",
              "metadata": {},
              "errorTraceType": "ansi",
              "type": "ipynbError",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-931624360125386>:2\u001b[0m\n",
              "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n",
              "\u001b[0;32m----> 2\u001b[0m predictionAndLabels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x\u001b[38;5;241m.\u001b[39mprediction, x\u001b[38;5;241m.\u001b[39moverall))\n",
              "\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MulticlassMetrics(predictionAndLabels)\n",
              "\u001b[1;32m      4\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mconfusionMatrix()\u001b[38;5;241m.\u001b[39mtoArray()\n",
              "\n",
              "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
            ]
          }
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "2023-05-14 - DBFS Example (1)",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}